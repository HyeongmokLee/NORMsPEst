import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import os
import random
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from mpl_toolkits.mplot3d import Axes3D
from collections import Counter
from matplotlib import pyplot as plt
from imblearn.under_sampling import RandomUnderSampler
from itertools import product
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import MinMaxScaler

# load data
data = pd.read_csv('./Data/Radon.txt', sep='  ', names=['G_label','Depth','T','pH','Eh','EC','DO','HCO3','Rn'])

# Concentration of Radon (Threshold = 100Bq/l)
dan_Rn = data[data['Rn'] == 1]
safe_Rn = data[data['Rn'] == 0]
dan_y = dan_Rn['Rn']
dan_x = dan_Rn.drop(['Rn'], axis=1)
safe_y = safe_Rn['Rn']
safe_x = safe_Rn.drop(['Rn'], axis=1)

# Noramalization (0 ~ 1) of data
def f(s):
    return (s-s.min())/(s.max()-s.min())
dan_x = dan_x.apply(f, axis=0)
safe_x = safe_x.apply(f, axis=0)

# Making cross validation dataset
X_train, safe_x1, Y_train, safe_y1 = train_test_split(safe_x, safe_y, test_size=ceil(len(safe_Rn)/5), random_state=20)
X_train, safe_x2, Y_train, safe_y2 = train_test_split(X_train, Y_train, test_size=ceil(len(safe_Rn)/5), random_state=20)
X_train, safe_x3, Y_train, safe_y3 = train_test_split(X_train, Y_train, test_size=ceil(len(safe_Rn)/5), random_state=20)
X_train, safe_x4, Y_train, safe_y4 = train_test_split(X_train, Y_train, test_size=ceil(len(safe_Rn)/5), random_state=20)
dan_x5 = X_train
dan_y5 = Y_train

# Making Stratified cross validation dataset
# SCV1
Test_x1 = pd.concat([safe_x1, dan_x1])
Test_y1 = pd.concat([safe_y1, dan_y1])
Train_x1 = pd.concat([safe_x2, safe_x3, safe_x4, safe_x5, dan_x2, dan_x3, dan_x4, dan_x5])
Train_y1 = pd.concat([safe_y2, safe_y3, safe_y4, safe_y5, dan_y2, dan_y3, dan_y4, dan_y5])

# SCV2
Test_x2 = pd.concat([safe_x2, dan_x2])
Test_y2 = pd.concat([safe_y2, dan_y2])
Train_x2 = pd.concat([safe_x1, safe_x3, safe_x4, safe_x5, dan_x1, dan_x3, dan_x4, dan_x5])
Train_y2 = pd.concat([safe_y1, safe_y3, safe_y4, safe_y5, dan_y1, dan_y3, dan_y4, dan_y5])

# SCV3
Test_x3 = pd.concat([safe_x3, dan_x3])
Test_y3 = pd.concat([safe_y3, dan_y3])
Train_x3 = pd.concat([safe_x1, safe_x2, safe_x4, safe_x5, dan_x1, dan_x2, dan_x4, dan_x5])
Train_y3 = pd.concat([safe_y1, safe_y2, safe_y4, safe_y5, dan_y1, dan_y2, dan_y4, dan_y5])

# SCV4
Test_x4 = pd.concat([safe_x4, dan_x4])
Test_y4 = pd.concat([safe_y4, dan_y4])
Train_x4 = pd.concat([safe_x1, safe_x2, safe_x3, safe_x5, dan_x1, dan_x2, dan_x3, dan_x5])
Train_y4 = pd.concat([safe_y1, safe_y2, safe_y3, safe_y5, dan_y1, dan_y2, dan_y3, dan_y5])

# SCV5
Test_x5 = pd.concat([safe_x5, dan_x5])
Test_y5 = pd.concat([safe_y5, dan_y5])
Train_x5 = pd.concat([safe_x1, safe_x2, safe_x3, safe_x4, dan_x1, dan_x2, dan_x3, dan_x4])
Train_y5 = pd.concat([safe_y1, safe_y2, safe_y3, safe_y4, dan_y1, dan_y2, dan_y3, dan_y4])

# SMOTE(Oversampling)
method = SMOTE()
X_smote1, y_smote1 = method.fit_resample(Train_x1, Train_y1)

# Random forest training and testing (SCV)
rf = RandomForestClassifier(n_estimators=300, max_leaf_nodes=70, n_jobs=-1)
rf.fit(X_smote1, y_smote1)
Pred_SCV1 = rf.predict(Test_x1)

# Performance of RF 
accuracy_rf = accuracy_score(Test_y1,Pred_SCV1)
f1_rf = f1_score(Test_y1,Pred_SCV1)
roc_score = roc_auc_score(Test_y1, Pred_SCV1)
print('Accuracy: ', accuracy_rf)
print('F1 score: ', f1_rf)
print('ROC AUC Score: ', roc_score)
